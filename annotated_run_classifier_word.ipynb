{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Background\n",
    "该文是对 [Bert in chinese text classification]( https://github.com/xieyufei1993/Bert-Pytorch-Chinese-TextClassification ) 此份源码的关键代码解读其一。该源码是基于 google Bert [官方](https://github.com/google-research/bert)推荐的 bert 的 pytorch 版本，[huggingface](https://github.com/huggingface/transformers) [v0.6.2](https://github.com/huggingface/transformers/releases/tag/v0.6.2) 或之前版本的实现（1.0 以后的版本变得差异较大，之前的版本更像是直接翻译 TF 的代码）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT finetuning runner.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import tokenization_word as tokenization\n",
    "from modeling import BertConfig, BertForSequenceClassification\n",
    "from optimization import BERTAdam\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "Bert 在中文是基于单字的，所以不需要分词。\n",
    "\n",
    "## 数据集\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每条数据\n",
    "可以发现它要求的输入分别是guid, text_a, text_b, label，其中text_b和label为可选参数。例如我们要做的是单个句子的分类任务，那么就不需要输入text_b；另外，在test样本中，我们便不需要输入lable.[ref](https://blog.csdn.net/weixin_37947156/article/details/84877254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProcessor\n",
    "DataPorcessor 将 raw text data 封装到 InputExample 中，一条数据对应一个 InputExample 实例。\n",
    "DataPorcessor 是所有数据载入模块的基类，根据不同的数据集构建不同的 processor。\"MrpcProcessor\" 等 processor 是 huggingface 给出的源码例子（该 processor 貌似是 Bert 官方给出的方式的pytorch版）, NewsProcessor 是我们要用到的数据集定制的 processor.\n",
    "\n",
    "DataProcessor方法：\n",
    "1. get_train_examples 读入训练集文本文件，每条数据对应一个 InputExample， 返回一个 InputExample 列表， 存储所有数据集 sample.\n",
    "2. get_dev_examples 读入验证集文本文件，每条数据对应一个 InputExample， 返回一个 InputExample 列表， 存储所有数据集 sample.\n",
    "3. get_labels 返回该数据集上的 label 集合， 列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        file_in = open(input_file, \"rb\")\n",
    "        lines = []\n",
    "        for line in file_in:\n",
    "            lines.append(line.decode(\"utf-8\").split(\"\\t\"))\n",
    "        return lines\n",
    "\n",
    "\n",
    "class NewsProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labels = set()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return list(self.labels)\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            self.labels.add(label)\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(line[3])\n",
    "            text_b = tokenization.convert_to_unicode(line[4])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
    "            \"dev_matched\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, tokenization.convert_to_unicode(line[0]))\n",
    "            text_a = tokenization.convert_to_unicode(line[8])\n",
    "            text_b = tokenization.convert_to_unicode(line[9])\n",
    "            label = tokenization.convert_to_unicode(line[-1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class ColaProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(line[3])\n",
    "            label = tokenization.convert_to_unicode(line[1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本表示\n",
    "将数据由自然语言文本转化为计算机可理解的特征（数字）。\n",
    "\n",
    "convert_examples_to_features 将 DataProcessor 存储的 InputExample list 转化为对应的文本表示 InputFeatures list.\n",
    " #The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "\n",
    "步骤： \n",
    "1. tokenize： tokenize text_a, 有text_b的话也一并处理，同时截断句子中超出 max_seq_length  的部分。当要有 text_b 的时候要有如下调用 _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3) 用来连接两个句子，如果连接出的句子长度超过最大值，则从较大的句子中截断超出的长度。\n",
    "    \n",
    "   tokenize 的细节：FullTokenizer 类, wordpiece 级别的 token 处理，同时也有文本清洗等与预处理操作。对每个句子 input = \"unaffable 1 unaffable 2\"\n",
    "          output = [[\"un\", \"##aff\", \"##able\"],[1],[\"un\", \"##aff\", \"##able\"],[2]].\n",
    "          \n",
    "> tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "> tokens_a = tokenizer.tokenize(example.text_a)\n",
    "> tokeniz\n",
    " \n",
    "2. 加入[cls],[sep]标志. [cls]:For classification tasks, the first vector (corresponding to [CLS]) is used as as the \"sentence vector\". Note that this only makes sense because the entire model is fine-tuned.\n",
    "3. 将得到的 token 由自然语言文本根据其在词汇库中的索引转化为数值形式的id(及文本表示).\n",
    "    tokenizer.convert_tokens_to_ids(tokens)：其中 tokenizer 会根据 self.vocab 中保存的词汇库返回 tokens 中每个 token 对应的id, self.vocab 是个字典，索引是词汇，值是词汇的id.\n",
    "4. 对每个句子补齐，补齐的内容都为 id 0.\n",
    "\n",
    "关键的变量：\n",
    "- features[]: 存储最后的结果，元素为  InputFeatures 对象，该对象的初始化方式为：InputFeatures(input_ids=input_ids, input_mask=input_mask,segment_ids=segment_ids,label_id=label_id)\n",
    "- tokens = [] ： 单个句子 tokenize 后又加入[seq]标识符, 补齐等处理后转化成 id 前的最终 token list.\n",
    "- segment_ids = [] : 标记是token是text_a, 还是 text_b，text_a 值为 0， text_b 为 1。因为对每个样本，我们把tokens_a, tokens_b 连在了一起，如：\"[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\" 中间的 [seq] 是两个句子的分隔。\n",
    "- input_ids = tokenizer.convert_tokens_to_ids(tokens)： tokens 转化成 ids\n",
    "- input_mask = [1] * len(input_ids) :简单的标识是否是 real token. “the mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.”\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [tokenization.printable_text(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                label_id=label_id))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "def copy_optimizer_params_to_model(named_params_model, named_params_optimizer):\n",
    "    \"\"\" Utility function for optimize_on_cpu and 16-bits training.\n",
    "        Copy the parameters optimized on CPU/RAM back to the model on GPU\n",
    "    \"\"\"\n",
    "    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n",
    "        if name_opti != name_model:\n",
    "            logger.error(\"name_opti != name_model: {} {}\".format(name_opti, name_model))\n",
    "            raise ValueError\n",
    "        param_model.data.copy_(param_opti.data)\n",
    "\n",
    "\n",
    "def set_optimizer_params_grad(named_params_optimizer, named_params_model, test_nan=False):\n",
    "    \"\"\" Utility function for optimize_on_cpu and 16-bits training.\n",
    "        Copy the gradient of the GPU parameters to the CPU/RAMM copy of the model\n",
    "    \"\"\"\n",
    "    is_nan = False\n",
    "    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n",
    "        if name_opti != name_model:\n",
    "            logger.error(\"name_opti != name_model: {} {}\".format(name_opti, name_model))\n",
    "            raise ValueError\n",
    "        if test_nan and torch.isnan(param_model.grad).sum() > 0:\n",
    "            is_nan = True\n",
    "        if param_opti.grad is None:\n",
    "            param_opti.grad = torch.nn.Parameter(param_opti.data.new().resize_(*param_opti.data.size()))\n",
    "        param_opti.grad.data.copy_(param_model.grad.data)\n",
    "    return is_nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main 函数\n",
    "以下所有代码均是 main 函数中的内容。为了注释和解析方便拆分成不同的 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Required parameters\n",
    "    parser.add_argument(\"--data_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    parser.add_argument(\"--bert_config_file\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The config json file corresponding to the pre-trained BERT model. \\n\"\n",
    "                             \"This specifies the model architecture.\")\n",
    "    parser.add_argument(\"--task_name\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The name of the task to train.\")\n",
    "    parser.add_argument(\"--vocab_file\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The vocabulary file that the BERT model was trained on.\")\n",
    "    parser.add_argument(\"--output_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--init_checkpoint\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        help=\"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "    parser.add_argument(\"--do_lower_case\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
    "    parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--do_train\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\"--train_batch_size\",\n",
    "                        default=32,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        default=5e-5,\n",
    "                        type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_train_epochs\",\n",
    "                        default=3.0,\n",
    "                        type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "    parser.add_argument(\"--save_checkpoints_steps\",\n",
    "                        default=1000,\n",
    "                        type=int,\n",
    "                        help=\"How often to save the model checkpoint.\")\n",
    "    parser.add_argument(\"--no_cuda\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "    parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument('--gradient_accumulation_steps',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"Number of updates steps to accumualte before performing a backward/update pass.\")\n",
    "    parser.add_argument('--optimize_on_cpu',\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to perform optimization and keep the optimizer averages on CPU\")\n",
    "    parser.add_argument('--fp16',\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "    parser.add_argument('--loss_scale',\n",
    "                        type=float, default=128,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    processors = {\n",
    "        \"cola\": ColaProcessor,\n",
    "        \"mnli\": MnliProcessor,\n",
    "        \"mrpc\": MrpcProcessor,\n",
    "        \"news\": NewsProcessor,\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 设置使用的 GPU  device = torch.device(\"cuda\", args.local_rank)\n",
    "model.to(device) 可将模型的计算挪到GPU上。\n",
    "\n",
    ">Via a string:\n",
    ">\n",
    "> \\>>>torch.device('cuda:0')\n",
    ">\n",
    ">device(type='cuda', index=0)\n",
    ">\n",
    "> \\>>>torch.device('cpu')\n",
    ">\n",
    ">device(type='cpu')\n",
    ">\n",
    ">\n",
    "> \\>>>torch.device('cuda')  # current cuda device\n",
    ">\n",
    ">device(type='cuda')\n",
    ">\n",
    "> \\>>>torch.device('cuda', 0)\n",
    ">device(type='cuda', index=0)\n",
    "\n",
    "> \\# Example of a function that takes in a torch.device\n",
    ">\n",
    "> \\>>>cuda1 = torch.device('cuda:1')\n",
    ">\n",
    "> \\>>>torch.randn((2,3), device=cuda1)\n",
    ">\n",
    "> \\# You can substitute the torch.device with a string\n",
    ">\n",
    "> \\>>>torch.randn((2,3), device='cuda:1')\n",
    "\n",
    "> \\>>>torch.randn((2,3), device=torch.device('cuda:1'))\n",
    "> \\>>>torch.randn((2,3), device='cuda:1')\n",
    "> \\>>>torch.randn((2,3), device=1)  # legacy\n",
    "\n",
    "\n",
    "2. gradient_accumulation_steps, Number of updates steps to accumualte before performing a backward/update pass. 是一个变相提高 batchsize 的方法，当计算机能力受限时，通过设置一个 gradient accumulation steps 相当于载入该值个 batchsize 累积 每个 batch 的 forward 计算出 loss 和 backprop 计算出的 gradient 后一次更新参数。而不是每个 batch 更新一次.\n",
    "\n",
    "代码中相关 commandline 参数 和 变量：\n",
    "**args.train_batch_size**\n",
    "在任何位置用到该参数之前先作如下操作更新自己：\n",
    "\n",
    "> args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n",
    "\n",
    "计算出在实现时每个 batch 的大小\n",
    "\n",
    "**args.gradient_accumulation_steps**\n",
    "间隔多少此 batch 更新一次参数。\n",
    "\n",
    "**args.num_train_epochs**\n",
    "计算多少个 epoch. 一个 epoch 是处理完一遍所有的数据。\n",
    "\n",
    "**num_train_steps**\n",
    "只和commandline 参数传入的 args.train_batch_size 有关，\n",
    "> args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n",
    "\n",
    "> num_train_steps = int(\n",
    "            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "    \n",
    "    等价于\n",
    "    \n",
    "> num_train_steps = len(train_examples) / args.train_batch_size(输入的) * args.num_train_epochs\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        # torch.distributed.init_process_group(backend='nccl')\n",
    "        if args.fp16:\n",
    "            logger.info(\"16-bits training currently not supported in distributed training\")\n",
    "            args.fp16 = False  # (see https://github.com/pytorch/pytorch/pull/13496)\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置随机数种子，使实验可复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "  \n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入长度\n",
    "1. bert_config.json 中 max_position_embeddings 为 512。\n",
    "2. max_seq_length： run_classification_word 的 commandline 参数，默认设置为 128。\"The maximum total input sequence length after WordPiece tokenization. equences longer than this will be truncated, and sequences shorter than this will be padded.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if not args.do_train and not args.do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "    bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "    if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "        raise ValueError(\n",
    "            \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
    "                args.max_seq_length, bert_config.max_position_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    task_name = args.task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    if args.do_train:\n",
    "        train_examples = processor.get_train_examples(args.data_dir)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "    \n",
    "    label_list = processor.get_labels()\n",
    "\n",
    "    print(\"label_list.size:%d\\n\" % (len(label_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型\n",
    "预先训练好的模型对我们来说是透明的，他是 google 开发好后存在了 pytorch_model.bin 中，而我们知道 Bert 对不同的任务有不同的架构，主要在顶层不同，而我们载入的模型只是除了顶层之外的通用部分，也就是（Todo 补图）这一部分. 载入了模型之后，我们需要定制化模型顶层。\n",
    "\n",
    "1. init_checkpoint： 预先训练好的模型路径， --init_checkpoint $BERT_PYTORCH_DIR/pytorch_model.bin \\\n",
    "2. args.fp16: 是否使用半精度，16位。\n",
    "3. When it comes to saving and loading models, there are three core functions to be familiar with:\n",
    "    - torch.save: Saves a serialized object to disk. This function uses Python?s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "    - torch.load: Uses pickle?s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "    - torch.nn.Module.load_state_dict: Loads a model?s parameter dictionary using a deserialized state_dict. For more information on state_dict, see What is a state_dict?.\n",
    "        \n",
    "What is a *state_dict*?\n",
    "\n",
    "In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the model?s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. Note that only layers with learnable parameters (convolutional layers, linear layers, etc.) and registered buffers (batchnorm?s running_mean) have entries in the model?s state_dict. Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer?s state, as well as the hyperparameters used.\n",
    "\n",
    "more about save/load model, click [here!](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Prepare model\n",
    "    model = BertForSequenceClassification(bert_config, len(label_list))\n",
    "    if args.init_checkpoint is not None:\n",
    "        model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    # if args.local_rank != -1:\n",
    "    # model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "    #                                                     output_device=args.local_rank)\n",
    "    # elif n_gpu > 1:\n",
    "    #    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器\n",
    "见（TODO）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Prepare optimizer\n",
    "    if args.fp16:\n",
    "        param_optimizer = [(n, param.clone().detach().to('cpu').float().requires_grad_()) \\\n",
    "                           for n, param in model.named_parameters()]\n",
    "    elif args.optimize_on_cpu:\n",
    "        param_optimizer = [(n, param.clone().detach().to('cpu').requires_grad_()) \\\n",
    "                           for n, param in model.named_parameters()]\n",
    "    else:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if n in no_decay], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = BERTAdam(optimizer_grouped_parameters,\n",
    "                         lr=args.learning_rate,\n",
    "                         warmup=args.warmup_proportion,\n",
    "                         t_total=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 和 eval\n",
    "## 训练数据的封装工具\n",
    "torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    global_step = 0\n",
    "    if args.do_train:\n",
    "        train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(train_data)\n",
    "        else:\n",
    "\n",
    "            train_sampler = RandomSampler(train_data)\n",
    "            # train_sampler = DistributedSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度保留，accumulation_steps 个 batch 后再更新\n",
    "每一个 batch 里的 loss 没有累加到下一个 batch, 而是立刻就调用 backwards(), 所以 loss 在每个 batch 都除一次 args.gradient_accumulation_steps 注意： 在这个项目中，每一个由 commandline 输入的 args.batch_size, 是指这么多个样本后会更新一次参数清零梯度。而每个 args.batch_size 的样本会被分成 args.gradient_accumula on_steps 份。每一份就是代码是实现时真正一次参与计算大 batch_size, args.batch_size 的取值是 [1,args.batch_size]。\n",
    "\n",
    "所以在下面代码中，累加了 args.gradient_accumulation_steps 个 batch 后才会调用 optimizer.step() 更新参数，此时相当于分批次共处理了 args.batch_size 个样本\n",
    "\n",
    "```\n",
    "for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    loss, _ = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "    if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "```\n",
    "\n",
    "区别于以下方法：\n",
    "\n",
    "```\n",
    "for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    loss += model(input_ids, segment_ids, input_mask, label_ids)[0]\n",
    "\n",
    "    if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        loss = 0\n",
    "```\n",
    "\n",
    "两种方法的区别在于，第二种更省内存.\n",
    "\n",
    "Ref:\n",
    "1. https://stackoverflow.com/questions/53331540/accumulating-gradients\n",
    "2. https://www.zhihu.com/question/303070254/answer/573037166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5d10e00f8837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-5d10e00f8837>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m## Required parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "        model.train()\n",
    "        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                loss, _ = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                if n_gpu > 1:\n",
    "                    loss = loss.mean()  # mean() to average on multi-gpu.\n",
    "                if args.fp16 and args.loss_scale != 1.0:\n",
    "                    # rescale loss for fp16 training\n",
    "                    # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "                    loss = loss * args.loss_scale\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    if args.fp16 or args.optimize_on_cpu:\n",
    "                        if args.fp16 and args.loss_scale != 1.0:\n",
    "                            # scale down gradients for fp16 training\n",
    "                            for param in model.parameters():\n",
    "                                param.grad.data = param.grad.data / args.loss_scale\n",
    "                        is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)\n",
    "                        if is_nan:\n",
    "                            logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "                            args.loss_scale = args.loss_scale / 2\n",
    "                            model.zero_grad()\n",
    "                            continue\n",
    "                        optimizer.step()\n",
    "                        copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "    if args.do_eval:\n",
    "        eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        if args.local_rank == -1:\n",
    "            eval_sampler = SequentialSampler(eval_data)\n",
    "        else:\n",
    "\n",
    "            eval_sampler = SequentialSampler(eval_data)\n",
    "            # eval_sampler = DistributedSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "        result = {'eval_loss': eval_loss,\n",
    "                  'eval_accuracy': eval_accuracy,\n",
    "                  'global_step': global_step,\n",
    "                  'loss': tr_loss / nb_tr_steps}\n",
    "\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
